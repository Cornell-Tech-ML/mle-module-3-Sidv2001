MAP

================================================================================
Parallel Accelerator Optimizing: Function tensor_map.<locals>.\_map,
/content/mle-module-3-Sidv2001/minitorch/fast_ops.py (154)  
================================================================================

Parallel loop listing for Function tensor_map.<locals>.\_map, /content/mle-module-3-Sidv2001/minitorch/fast_ops.py (154)
-------------------------------------------------------------------------|loop #ID
def \_map( |
out: Storage, |
out_shape: Shape, |
out_strides: Strides, |
in_storage: Storage, |
in_shape: Shape, |
in_strides: Strides, |
) -> None: | # TODO: Implement for Task 3.1. |
for o in prange(len(out)):---------------------------------------| #0
out_index: Index = np.zeros_like(out_shape) |
in_index: Index = np.zeros_like(in_shape) |
to_index(o, out_shape, out_index) |
broadcast_index(out_index, out_shape, in_shape, in_index) |
i = index_to_position(in_index, in_strides) |
out[o] = fn(in_storage[i]) |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #0).

---

## ----------------------------- Before Optimisation ------------------------------

------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.

---

---

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found
None

ZIP

================================================================================
Parallel Accelerator Optimizing: Function tensor_zip.<locals>.\_zip,
/content/mle-module-3-Sidv2001/minitorch/fast_ops.py (196)  
================================================================================

Parallel loop listing for Function tensor_zip.<locals>.\_zip, /content/mle-module-3-Sidv2001/minitorch/fast_ops.py (196)
-------------------------------------------------------------------------------|loop #ID
def \_zip( |
out: Storage, |
out_shape: Shape, |
out_strides: Strides, |
a_storage: Storage, |
a_shape: Shape, |
a_strides: Strides, |
b_storage: Storage, |
b_shape: Shape, |
b_strides: Strides, |
) -> None: | # TODO: Implement for Task 3.1. |
for o in prange(len(out)): # prange is used for parallel execution----| #1
out_index = np.zeros_like(out_shape) |
a_index = np.zeros_like(a_shape) |
b_index = np.zeros_like(b_shape) |
to_index(o, out_shape, out_index) |
broadcast_index(out_index, out_shape, a_shape, a_index) |
broadcast_index(out_index, out_shape, b_shape, b_index) |
a_pos = index_to_position(a_index, a_strides) |
b_pos = index_to_position(b_index, b_strides) |
out[o] = fn(a_storage[a_pos], b_storage[b_pos]) |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #1).

---

## ----------------------------- Before Optimisation ------------------------------

------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.

---

---

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found
None

REDUCE

================================================================================
Parallel Accelerator Optimizing: Function tensor_reduce.<locals>.\_reduce,
/content/mle-module-3-Sidv2001/minitorch/fast_ops.py (241)  
================================================================================

Parallel loop listing for Function tensor_reduce.<locals>.\_reduce, /content/mle-module-3-Sidv2001/minitorch/fast_ops.py (241)
-------------------------------------------------------------|loop #ID
def \_reduce( |
out: Storage, |
out_shape: Shape, |
out_strides: Strides, |
a_storage: Storage, |
a_shape: Shape, |
a_strides: Strides, |
reduce_dim: int, |
) -> None: | # TODO: Implement for Task 3.1. |
|
for o in prange(len(out)):---------------------------| #2
out_index: Index = np.zeros_like(out_shape) |
reduce_size = a_shape[reduce_dim] |
to_index(o, out_shape, out_index) |
res = index_to_position(out_index, a_strides) |
dim_stride = a_strides[reduce_dim] |
for s in range(reduce_size): |
j = res + (s \* dim_stride) |
out[o] = fn(out[o], a_storage[j]) |
--------------------------------- Fusing loops ---------------------------------
Attempting fusion of parallel loops (combines loops with similar properties)...
Following the attempted fusion of parallel for-loops there are 1 parallel for-
loop(s) (originating from loops labelled: #2).

---

## ----------------------------- Before Optimisation ------------------------------

------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.

---

---

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found
None

MATRIX MULTIPLY

================================================================================
Parallel Accelerator Optimizing: Function \_tensor_matrix_multiply,
/content/mle-module-3-Sidv2001/minitorch/fast_ops.py (265)  
================================================================================

Parallel loop listing for Function \_tensor\*matrix_multiply, /content/mle-module-3-Sidv2001/minitorch/fast_ops.py (265)
------------------------------------------------------------------------------------------------------------------|loop #ID
def \_tensor_matrix_multiply( |
out: Storage, |
out_shape: Shape, |
out_strides: Strides, |
a_storage: Storage, |
a_shape: Shape, |
a_strides: Strides, |
b_storage: Storage, |
b_shape: Shape, |
b_strides: Strides, |
) -> None: |
""" |
NUMBA tensor matrix multiply function. |
|
Should work for any tensor shapes that broadcast as long as |
|
`                                                                                                     | 
    assert a_shape[-1] == b_shape[-2]                                                                             |` |
|
Optimizations: |
|

- Outer loop in parallel |
  _ No index buffers or function calls |
  _ Inner loop should have no global writes, 1 multiply. |
  |
  |
  Args: |
  out (Storage): storage for `out` tensor |
  out*shape (Shape): shape for `out` tensor |
  out_strides (Strides): strides for `out` tensor |
  a_storage (Storage): storage for `a` tensor |
  a_shape (Shape): shape for `a` tensor |
  a_strides (Strides): strides for `a` tensor |
  b_storage (Storage): storage for `b` tensor |
  b_shape (Shape): shape for `b` tensor |
  b_strides (Strides): strides for `b` tensor |
  |
  Returns: |
  None : Fills in `out` |
  """ |
  a_batch_stride = a_strides[0] if a_shape[0] > 1 else 0 |
  b_batch_stride = b_strides[0] if b_shape[0] > 1 else 0 |
  |
  | # TODO: Implement for Task 3.2. |
  assert a_shape[-1] == b_shape[-2] |
  for o in prange(len(out)):------------------------------------------------------------------------------------| #3 # o_batch_id = o // out_strides[0] | # o_row_id = (o % out_strides[0]) // out_strides[1] | # o_col_id = o % out_shape[-1] |
  o_batch_id = o // (out_shape[-1] * out*shape[-2]) |
  out_row_id = (o % (out_shape[-1] * out*shape[-2])) // out_shape[-1] |
  out_col_id = o % out_shape[-1] |
  a_batch_row = o_batch_id * a*batch_stride + out_row_id * a*strides[-2] |
  b_batch_col = o_batch_id * b*batch_stride + out_col_id * b*strides[-1] |
  res = 0 |
  for i in range(a_shape[-1]): |
  res += a_storage[a_batch_row + (i * a*strides[-1])] * b*storage[b_batch_col + (i * b_strides[-2])] |
  out[o] = res |
  --------------------------------- Fusing loops ---------------------------------
  Attempting fusion of parallel loops (combines loops with similar properties)...
  Following the attempted fusion of parallel for-loops there are 1 parallel for-
  loop(s) (originating from loops labelled: #3).

---

## ----------------------------- Before Optimisation ------------------------------

------------------------------ After Optimisation ------------------------------
Parallel structure is already optimal.

---

---

---------------------------Loop invariant code motion---------------------------
Allocation hoisting:
No allocation hoisting found
None